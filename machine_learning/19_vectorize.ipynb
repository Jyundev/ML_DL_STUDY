{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer\n",
    "\n",
    "CountVectorizer는 자연어 처리(NLP)에서 사용되는 기능 추출 기술로, 텍스트 문서를 숫자 특성 벡터로 변환하는 것입니다. \n",
    "\n",
    "이는 파이썬의 인기 있는 머신러닝 라이브러리인 scikit-learn에 포함되어 있습니다.\n",
    "\n",
    "CountVectorizer는 텍스트를 개별 단어나 용어로 토큰화하고, 각 용어의 빈도를 계산함으로써 작동합니다. \n",
    "\n",
    "이는 말뭉치(corpus)의 모든 고유한 용어로 어휘를 구성하고, 각 용어에 고유한 인덱스를 할당합니다. \n",
    "\n",
    "결과적으로 생성된 특성 벡터는 문서에서 각 용어의 존재 여부를 나타냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = [\n",
    "'나는 나로 살기로 했다',\n",
    "'인간이란 다 슬퍼하는 동안에도',\n",
    "'나는 즐거워하기로 했다',\n",
    "'무엇이든지 어떻게든 될 것이라고' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "t = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['나', '는', '나로', '살기', '로', '했다'],\n",
       " ['인간', '이란', '다', '슬퍼하는', '동안', '에도'],\n",
       " ['나', '는', '즐거워하기로', '했다'],\n",
       " ['무엇', '이든지', '어떻', '게', '든', '될', '것', '이라고']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_tokens = [t.morphs(row) for row in content]\n",
    "content_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' 나 는 나로 살기 로 했다',\n",
       " ' 인간 이란 다 슬퍼하는 동안 에도',\n",
       " ' 나 는 즐거워하기로 했다',\n",
       " ' 무엇 이든지 어떻 게 든 될 것 이라고']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents_for_vectorize = []\n",
    "\n",
    "for content in content_tokens:\n",
    "    sentence = ''\n",
    "    for word in content:\n",
    "        sentence = sentence + ' ' + word\n",
    "\n",
    "    contents_for_vectorize.append(sentence)\n",
    "\n",
    "contents_for_vectorize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(contents_for_vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 13)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples, num_features = X.shape\n",
    "\n",
    "num_samples, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['나로', '동안', '무엇', '살기', '슬퍼하는', '어떻', '에도', '이든지', '이라고', '이란',\n",
       "       '인간', '즐거워하기로', '했다'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [1, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' 나 는 즐거워하는 인간 이다']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_post = ['나는 즐거워하는 인간이다']\n",
    "new_post_tokens = [t.morphs(row) for row in new_post]\n",
    "\n",
    "new_post_for_vectorize = []\n",
    "\n",
    "for content in new_post_tokens:\n",
    "    sentence = ''\n",
    "    for word in content:\n",
    "        sentence = sentence + ' ' + word\n",
    "\n",
    "    new_post_for_vectorize.append(sentence)\n",
    "\n",
    "new_post_for_vectorize    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_post_vec = vectorizer.transform(new_post_for_vectorize)\n",
    "new_post_vec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "def dist_raw(v1, v2):\n",
    "    delta = v1-v2\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0, 2.0, 1.7320508075688772, 2.23606797749979]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = [dist_raw(each, new_post_vec) for each in X]\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bset post is  2 , dist =  1.7320508075688772\n",
      "Test post is  ['나는 즐거워하는 인간이다']\n",
      "Bset dist post is  즐거워하는\n"
     ]
    }
   ],
   "source": [
    "print('Bset post is ',dist.index(min(dist)), ', dist = ',min(dist))\n",
    "print('Test post is ',new_post)\n",
    "print('Bset dist post is ',content[dist.index(min(dist))])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF vectorize\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency)는 문서 내에서 단어의 중요성을 평가하는 데 사용되는 통계적인 가중치입니다. \n",
    "\n",
    "TF-IDF 벡터화는 문서 집합을 벡터로 변환하는 방법 중 하나입니다. 이 방법은 텍스트 분석과 정보 검색에서 널리 사용됩니다.\n",
    "\n",
    "TF-IDF 벡터화는 각 문서에서 각 용어의 TF-IDF 값을 계산하여 문서를 표현합니다.\n",
    "\n",
    "TF(Term Frequency)는 특정 용어의 문서 내 출현 빈도를 나타내며, IDF(Inverse Document Frequency)는 해당 용어의 문서 전체에서의 중요성을 측정합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = 1, decode_error = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(contents_for_vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 13)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples, num_features = X.shape\n",
    "\n",
    "num_samples, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.61761437, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.4472136 , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.5       ],\n",
       "       [0.61761437, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.4472136 , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.5       ],\n",
       "       [0.        , 0.4472136 , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.5       ],\n",
       "       [0.        , 0.        , 0.        , 0.5       ],\n",
       "       [0.        , 0.4472136 , 0.        , 0.        ],\n",
       "       [0.        , 0.4472136 , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.78528828, 0.        ],\n",
       "       [0.48693426, 0.        , 0.6191303 , 0.        ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_post_vec = vectorizer.transform(new_post_for_vectorize)\n",
    "new_post_vec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_norm(v1, v2):\n",
    "    v1_norm = v1 / sp.linalg.norm(v1.toarray())\n",
    "    v2_norm = v2 / sp.linalg.norm(v2.toarray())\n",
    "\n",
    "    delta = v1_norm-v2_norm\n",
    "    \n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bset post is  1 , dist =  1.0514622242382672\n",
      "Test post is  ['나는 즐거워하는 인간이다']\n",
      "Bset dist post is  는\n"
     ]
    }
   ],
   "source": [
    "dist = [dist_norm(each, new_post_vec) for each in X]\n",
    "\n",
    "print('Bset post is ',dist.index(min(dist)), ', dist = ',min(dist))\n",
    "print('Test post is ',new_post)\n",
    "print('Bset dist post is ',content[dist.index(min(dist))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
